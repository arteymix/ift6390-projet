{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auteurs:** Guillaume Poirier-Morency et Gabriel Lemyre\n",
    "\n",
    "Chaque modèle est présenté successivement, entraîné et finalement testés selon les meilleurs paramètres obtenus par le processus de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données de salaire est déjà séparé en deux ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_dtype = OrderedDict([('age', 'int'), \n",
    "                            ('workclass', 'category'), \n",
    "                            ('financial_weight', 'int'), \n",
    "                            ('education', 'category'), \n",
    "                            ('education_code', 'int'),\n",
    "                            ('marital_status', 'category'), \n",
    "                            ('occupation', 'category'),\n",
    "                            ('relationship', 'category'),\n",
    "                            ('race', 'category'),\n",
    "                            ('sex', 'category'),\n",
    "                            ('capital_gain', 'int'),\n",
    "                            ('capital_loss', 'int'),\n",
    "                            ('hours_per_week', 'int'),\n",
    "                            ('native_country', 'category'),\n",
    "                            ('target', 'category')])\n",
    "salary_continuous_columns = ['age', 'financial_weight', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "salary_categorical_columns = ['workclass', 'education', 'education_code', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "salary_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', sep=', ', engine='python', names=salary_dtype.keys(), dtype=salary_dtype, na_values=['?'])\n",
    "salary_test = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test', sep=', ', engine='python', skiprows=[0], names=salary_dtype.keys(), dtype=salary_dtype, na_values=['?'])\n",
    "\n",
    "salary_data[salary_categorical_columns] = salary_data[salary_categorical_columns].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "salary_test[salary_categorical_columns] = salary_data[salary_categorical_columns].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "identity = lambda x: x\n",
    "salary_transform = {\n",
    "    'age': identity,\n",
    "    'workclass': LabelEncoder().fit(salary_data.workclass).transform,\n",
    "    'financial_weight': identity,\n",
    "    'education': LabelEncoder().fit(salary_data.education).transform,\n",
    "    'education_code': identity,\n",
    "    'marital_status': LabelEncoder().fit(salary_data.marital_status).transform,\n",
    "    'occupation': LabelEncoder().fit(salary_data.occupation).transform,\n",
    "    'relationship': LabelEncoder().fit(salary_data.relationship).transform,\n",
    "    'race': LabelEncoder().fit(salary_data.race).transform,\n",
    "    'sex': LabelEncoder().fit(salary_data.sex).transform,\n",
    "    'capital_gain': identity,\n",
    "    'capital_loss': identity,\n",
    "    'hours_per_week': identity,\n",
    "    'native_country': LabelEncoder().fit(salary_data.native_country).transform,\n",
    "    'target': lambda x: LabelBinarizer().fit_transform(x).ravel()}\n",
    "salary_data = salary_data.transform(salary_transform)\n",
    "salary_test = salary_test.transform(salary_transform)\n",
    "\n",
    "salary_train_X, salary_train_Y = salary_data.iloc[:,:len(salary_dtype)-1], salary_data['target']\n",
    "salary_test_X, salary_test_Y = salary_test.iloc[:,:len(salary_dtype)-1], salary_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "sb.pairplot(salary_data, vars=salary_continuous_columns, hue='target')\n",
    "plt.savefig('figures/salary-pair-plot', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder, LabelEncoder, FunctionTransformer\n",
    "\n",
    "salary_preprocessing_pipeline = Pipeline([('imputer', Imputer(strategy='mean')),\n",
    "                                          ('cat-to-one-hot', OneHotEncoder(categorical_features=[salary_data.columns.get_loc(c) for c in salary_categorical_columns],\n",
    "                                                                           n_values=salary_data[salary_categorical_columns].nunique().as_matrix(),\n",
    "                                                                           handle_unknown='ignore',\n",
    "                                                                           sparse=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise un état déterministe pour la routine `train_test_split` afin de s'assurer de ne jamais toucher l'ensemble de test avant la toute fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = fetch_mldata('mnist-original')\n",
    "mnist_train_X, mnist_test_X, mnist_train_Y, mnist_test_Y = train_test_split(mnist_data['data'], mnist_data['target'], random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieur de Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary\n",
    "\n",
    "Pour classer les features catégoriques du dataset de salaires, on les convertit en one-hot et on utilise un classifier naïf ad-hoc avec densité de Bernouilli. On considère ensuite la probabilité suivante: $\\Pr [c \\mid x_{cont},x_{cat}] = \\frac{\\Pr[X_{cont} X_{cat} \\mid c]\\Pr[c]}{\\Pr[X_{cont}] \\Pr[X_{cat}]}$.\n",
    "\n",
    "Avec l'hypothèse naïve $\\Pr[X_{cont},X_{cat}] = \\Pr[X_{cont}] \\Pr[X_{cat}]$ et en passant par le logarithme:\n",
    "\n",
    "$\\implies \\log \\Pr[X_{cont} \\mid c] + \\log \\Pr[X_{cat} \\mid c] + \\log \\Pr[c] - (\\log \\Pr[X_{cont}] + \\log \\Pr[X_{cat}])$\n",
    "\n",
    "Puisque la probabilité finale combine des densités (i.e. continues) et des masses (i.e. discrètes), on pondère chaque classifieur par un hyper-paramètre $\\lambda$:\n",
    "\n",
    "$\\implies \\lambda \\log \\Pr[X_{cont} \\mid c] + (1 - \\lambda) \\log \\Pr[X_{cat} \\mid c] + \\log \\Pr[c] - (\\lambda \\log \\Pr[X_{cont}] + (1 - \\lambda) \\log \\Pr[X_{cat}])$\n",
    "$\\implies \\lambda \\log \\Pr[c \\mid X_{cont}] + (1 - \\lambda) \\log \\Pr[c \\mid X_{cat}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes mixte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class MixedNB(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Mixed weighted gaussian and binomial naive Bayes classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, cont, cat, alpha=1.0, lambda_=0.5):\n",
    "        self.cont = cont\n",
    "        self.cat = cat\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.gnb = GaussianNB()\n",
    "        self.bnb = BernoulliNB(alpha)\n",
    "    def get_params(self, deep=False):\n",
    "        return {'cont': self.cont, 'cat': self.cat, 'alpha': self.bnb.alpha, 'lambda_': self.lambda_}\n",
    "    def set_params(self, **parameters):\n",
    "        for name, val in parameters.items():\n",
    "            setattr(self, name, val)\n",
    "        self.bnb.set_params(alpha=self.alpha)\n",
    "        return self\n",
    "    def fit(self, X, y):\n",
    "        self.gnb.fit(X[:,self.cont], y)\n",
    "        self.bnb.fit(X[:,self.cat], y)\n",
    "    def predict_log_proba(self, X):\n",
    "        return self.lambda_ * self.gnb.predict_log_proba(X[:,self.cont]) + (1 - self.lambda_) * self.bnb.predict_log_proba(X[:,self.cat])\n",
    "    def predict_proba(self, X):\n",
    "        return np.exp(self.predict_log_proba(X))\n",
    "    def predict(self, X):\n",
    "        return self.gnb.classes_[np.argmax(self.predict_log_proba(X), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mnb_salary_param_grid = {'mixed_nb__lambda_': np.linspace(0, 1), \n",
    "                         'mixed_nb__alpha': np.linspace(1, 10)}\n",
    "mnb_salary = GridSearchCV(Pipeline([('pre', salary_preprocessing_pipeline), \n",
    "                                    ('mixed_nb', MixedNB(cont=np.arange(115, 120), cat=np.arange(115)))]), param_grid=mnb_salary_param_grid, scoring='accuracy', n_jobs=16, return_train_score=True)\n",
    "mnb_salary.fit(salary_train_X, salary_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(mnb_salary.cv_results_)\n",
    "r = r.groupby('param_mixed_nb__lambda_').apply(lambda x: x.sort_values(by='mean_test_score', ascending=False).head(1))\n",
    "plt.plot(r.param_mixed_nb__lambda_, 1 - r.mean_train_score, label='Erreur d\\'entraînement')\n",
    "plt.plot(r.param_mixed_nb__lambda_, 1 - r.mean_test_score, label='Erreur de validation')\n",
    "plt.title('Courbe d\\'apprentissage du Bayes mixte sur les données de salaire')\n",
    "plt.xlabel('Poids de chaque classifieur')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.savefig('figures/mixed-naive-bayes-salary-learning-curve-lambda', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(mnb_salary.cv_results_)\n",
    "r = r.groupby('param_mixed_nb__alpha').apply(lambda x: x.sort_values(by='mean_test_score', ascending=False).head(1))\n",
    "plt.plot(r.param_mixed_nb__alpha, 1 - r.mean_train_score, label='Erreur d\\'entraînement')\n",
    "plt.plot(r.param_mixed_nb__alpha, 1 - r.mean_test_score, label='Erreur de validation')\n",
    "plt.title('Courbe d\\'apprentissage du Bayes mixte sur les données de salaire')\n",
    "plt.xlabel('Lissage laplacien')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.savefig('figures/mixed-naive-bayes-salary-learning-curve-alpha', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_mnist = GaussianNB()\n",
    "cross_val_score(gnb_mnist, mnist_train_X, mnist_train_Y, scoring='accuracy', n_jobs=16).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_mnist.fit(mnist_train_X, mnist_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "bnb_mnist_param_grid = {'bnb__alpha': range(1, 10)}\n",
    "bnb_mnist = GridSearchCV(Pipeline([('binarize', Binarizer()), \n",
    "                                   ('bnb', BernoulliNB())]), param_grid=bnb_mnist_param_grid, scoring='accuracy', n_jobs=16, return_train_score=True)\n",
    "bnb_mnist.fit(mnist_train_X, mnist_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(bnb_mnist.cv_results_)\n",
    "plt.plot(r.param_bnb__alpha, 1 - r.mean_train_score, label='Erreur d\\'entraînement')\n",
    "plt.plot(r.param_bnb__alpha, 1 - r.mean_test_score, label='Erreur de validation')\n",
    "plt.title('Courbe d\\'apprentissage du Bayes à noyau de Bernoulli sur les données de MNIST')\n",
    "plt.xlabel('Lissage laplacien')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.savefig('figures/bernoulli-naive-bayes-mnist-learning-curve-alpha', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_salary_param_grid = {'dtc__max_depth': range(1, 20), \n",
    "                         'dtc__min_samples_leaf': range(1, 30)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtc_salary = GridSearchCV(Pipeline([('pre', salary_preprocessing_pipeline), \n",
    "                                    ('dtc', DecisionTreeClassifier())]), param_grid=dtc_salary_param_grid, scoring='accuracy', n_jobs=16, return_train_score=True)\n",
    "dtc_salary.fit(salary_train_X, salary_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(dtc_salary.cv_results_)\n",
    "r = r.groupby('param_dtc__max_depth').apply(lambda x: x.sort_values(by='mean_test_score', ascending=False).head(1))\n",
    "plt.plot(r.param_dtc__max_depth, 1 - r.mean_train_score, label='Erreur d\\'entraînement')\n",
    "plt.plot(r.param_dtc__max_depth, 1 - r.mean_test_score, label='Erreur de validation')\n",
    "plt.title('Courbe d\\'apprentissage des arbres de décisions sur salary')\n",
    "plt.xlabel('Profondeur maximale')\n",
    "plt.ylabel('Erreur')\n",
    "plt.xticks(dtc_salary_param_grid['dtc__max_depth'])\n",
    "plt.legend()\n",
    "plt.savefig('figures/decision-tree-salary-learning-curve-max-depth', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(dtc_salary.cv_results_)\n",
    "r = r.groupby('param_dtc__min_samples_leaf').apply(lambda x: x.sort_values(by='mean_test_score', ascending=False).head(1))\n",
    "plt.plot(r.param_dtc__min_samples_leaf, 1 - r.mean_train_score, label='Erreur d\\'entraînement')\n",
    "plt.plot(r.param_dtc__min_samples_leaf, 1 - r.mean_test_score, label='Erreur de validation')\n",
    "plt.title('Courbe d\\'apprentissage des arbres de décisions sur salary')\n",
    "plt.xlabel('Nombre minimal d\\'échantillons aux feuilles')\n",
    "plt.ylabel('Erreur')\n",
    "plt.xticks(dtc_salary_param_grid['dtc__min_samples_leaf'])\n",
    "plt.legend()\n",
    "plt.savefig('figures/decision-tree-salary-learning-curve-min-samples-leaf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_mnist_param_grid = {'dtc__max_depth': range(1, 30), \n",
    "                        'dtc__min_samples_leaf': range(1, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtc_mnist = GridSearchCV(Pipeline([('dtc', DecisionTreeClassifier())]), param_grid=dtc_mnist_param_grid, scoring='accuracy', n_jobs=16, return_train_score=True)\n",
    "dtc_mnist.fit(mnist_train_X, mnist_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(dtc_mnist.cv_results_)\n",
    "r = r.groupby('param_dtc__max_depth').apply(lambda x: x.sort_values(by='mean_test_score', ascending=False).head(1))\n",
    "plt.plot(r.param_dtc__max_depth, 1 - r.mean_train_score, label='Erreur d\\'entraînement')\n",
    "plt.plot(r.param_dtc__max_depth, 1 - r.mean_test_score, label='Erreur de validation')\n",
    "plt.title('Courbe d\\'apprentissage des arbres de décisions sur MNIST')\n",
    "plt.xlabel('Profondeur maximale')\n",
    "plt.ylabel('Erreur')\n",
    "plt.xticks(dtc_mnist_param_grid['dtc__max_depth'])\n",
    "plt.legend()\n",
    "plt.savefig('figures/decision-tree-mnist-learning-curve-max-depth', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(dtc_mnist.cv_results_)\n",
    "r = r.groupby('param_dtc__min_samples_leaf').apply(lambda x: x.sort_values(by='mean_test_score', ascending=False).head(1))\n",
    "plt.plot(r.param_dtc__min_samples_leaf, 1 - r.mean_train_score, label='Erreur d\\'entraînement')\n",
    "plt.plot(r.param_dtc__min_samples_leaf, 1 - r.mean_test_score, label='Erreur de validation')\n",
    "plt.title('Courbe d\\'apprentissage des arbres de décisions sur MNIST')\n",
    "plt.xlabel('Nombre minimal d\\'échantillons aux feuilles')\n",
    "plt.ylabel('Erreur')\n",
    "plt.xticks(dtc_mnist_param_grid['dtc__min_samples_leaf'])\n",
    "plt.legend()\n",
    "plt.savefig('figures/decision-tree-mnist-learning-curve-min-samples-leaf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron multi-couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.losses import categorical_crossentropy, binary_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adagrad, Adadelta\n",
    "from keras.regularizers import l1_l2, l1, l2\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_mlp = Sequential()\n",
    "salary_mlp.add(Dense(units=500, activation='relu', input_dim=120))\n",
    "salary_mlp.add(Dense(units=2, activation='softmax'))\n",
    "salary_mlp.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "salary_mlp_history = salary_mlp.fit(salary_preprocessing_pipeline.fit_transform(salary_train_X), \n",
    "                                    to_categorical(salary_train_Y), \n",
    "                                    validation_split=0.33, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Courbe d\\'apprentissage du perceptron multi-couche sur les données de salaire')\n",
    "plt.title('Une couche cachée de 50 neurones')\n",
    "plt.plot(1 - np.array(salary_mlp_history.history['acc']), label='Erreur d\\'entraînement')\n",
    "plt.plot(1 - np.array(salary_mlp_history.history['val_acc']), label='Erreur de validation')\n",
    "plt.xlabel('Époque')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.savefig('figures/multilayer-perceptron-salary-learning-curve-epoch', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_mlp = Sequential()\n",
    "mnist_mlp.add(Dense(units=512, activation='relu', input_dim=784))\n",
    "mnist_mlp.add(Dropout(0.1))\n",
    "mnist_mlp.add(Dense(units=10, activation='softmax'))\n",
    "mnist_mlp.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_mlp_history = mnist_mlp.fit(mnist_train_X, to_categorical(mnist_train_Y), validation_split=0.33, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Courbe d\\'apprentissage du perceptron multi-couche sur MNIST')\n",
    "plt.title('Une couche cachée de 512 neurones et 0.1 dropout')\n",
    "plt.plot(1 - np.array(mnist_mlp_history.history['acc']), label='Erreur d\\'entraînement')\n",
    "plt.plot(1 - np.array(mnist_mlp_history.history['val_acc']), label='Erreur de validation')\n",
    "plt.xlabel('Époque')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.savefig('figures/multilayer-perceptron-mnist-learning-curve-epoch', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de neurones convolutif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_cnn = Sequential()\n",
    "mnist_cnn.add(Reshape((28,28,1), input_shape=(784,)))\n",
    "mnist_cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "mnist_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "mnist_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "mnist_cnn.add(Flatten())\n",
    "mnist_cnn.add(Dropout(0.3))\n",
    "mnist_cnn.add(Dense(128, activation='relu'))\n",
    "mnist_cnn.add(Dropout(0.3))\n",
    "mnist_cnn.add(Dense(10, activation='softmax'))\n",
    "mnist_cnn.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adadelta(),\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_cnn_history = mnist_cnn.fit(mnist_train_X, to_categorical(mnist_train_Y), validation_split=0.33, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Courbe d\\'apprentissage du réseau de neurones convolutif sur MNIST\\n'\n",
    "            'Convolution 3x3 de 32 features, convolution 3x3 de 64 features,\\npooling, ropout 0.3, 128 neurones cachés et dropout 0.3')\n",
    "plt.plot(1 - np.array(mnist_cnn_history.history['acc']), label='Erreur d\\'entraînement')\n",
    "plt.plot(1 - np.array(mnist_cnn_history.history['val_acc']), label='Erreur de validation')\n",
    "plt.xlabel('Époque')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.savefig('figures/convolutional-neural-network-mnist-learning-curve-epoch', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "Ici, on trouve le code pour les tests finaux qui ont été effectués à la toute fin, indépendament du processus de validation afin d'avoir la meilleure idée possible de la performance de généralisation de chaque modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieurs Bayésiens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(salary_test_Y, mnb_salary.predict(salary_test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(mnist_test_Y, gnb_mnist.predict(mnist_test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(mnist_test_Y, bnb_mnist.predict(mnist_test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbres de décisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(salary_test_Y, dtc_salary.predict(salary_test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(mnist_test_Y, dtc_mnist.predict(mnist_test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_mlp.evaluate(salary_preprocessing_pipeline.fit_transform(salary_test_X), to_categorical(salary_test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_mlp.evaluate(mnist_test_X, to_categorical(mnist_test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_cnn.evaluate(mnist_test_X, to_categorical(mnist_test_Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2",
   "language": "python",
   "name": "python3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
